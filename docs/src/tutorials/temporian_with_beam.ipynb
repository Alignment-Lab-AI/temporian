{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Temporian on Apache Beam\n",
    "\n",
    "This guide shows how to run Temporian programs on large datasets with Beam.\n",
    "\n",
    "**WARNING:** Temporian with Apache Beam is experimental. The API might change, some optimizations are not implemented, and some operators are not available.\n",
    "\n",
    "The reader is assumed to be familiar with Temporian in-process execution. Please read the [3 Minutes to Temporian](../3_minutes), or the [User Guide](../user_guide) before.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The Temporian [User Guide](user_guide.md) teaches how to run a Temporian program in-process using Python. This approach is ideal for quick experimentation and for datasets that are small enough to fit in a single computer. However, for large datasets with billions or even trillions of events, this approach will run out of memory. Instead, you can execute Temporian programs on large datasets using [Apache Beam](https://beam.apache.org/).\n",
    "\n",
    "**Info:** Apache Beam is a library for large-scale distributed computation. \n",
    "\n",
    "Executing a Temporian program in-process and executing it with  Beam are very similar. There are only two things to watch out for:\n",
    "\n",
    "- The Temporian program needs to be defined in [graph execution mode](user_guide.md#eager-mode-vs-graph-mode). Eager execution mode is not compatible with Beam.\n",
    "- Data set I/O needs to be Beam-compatible. To enable this, replace the normal dataset I/O with their counterparts in `temporian.beam` (for example `temporian.from_csv` will become `temporian.beam.from_csv`). Alternatively, you can implement your own IO code using [Beam IO connectors](https://beam.apache.org/documentation/io/connectors/). Dataset I/O with Beam will be explained in more detail in [this section](#input-and-output-data).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "\n",
    "We need to install Temporian and Apache Beam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install temporian[beam] -q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example\n",
    "\n",
    "In this example, we run a Temporian program on a csv file both with the in-process and with the Beam approaches.\n",
    "\n",
    "First, we create a small csv file containing our input data. This dataset contains 5 events with two features (`a` and `b`). Individual events are stored as separate rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile input.csv\n",
    "timestamp,a,b\n",
    "1,x,1\n",
    "2,y,2\n",
    "13,z,3\n",
    "14,x,2\n",
    "15,y,1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In practice, large datasets should be divided into multiple files called “shards” to facilitate the distribution of the work among machines. In this case, dataset paths can be a glob expression e.g., `input-*.csv`. For more information, see the \"Speed-up pipeline\" section.\n",
    "\n",
    "**Note 2:** In this file, each event is presented in a separate row. For dataset formats that support it, it is more efficient to group examples of the same index key together. For more information, see the \"Speed-up pipeline\" section.\n",
    "\n",
    "The following code processes our csv file and outputs the result in the `output_in_process.csv` csv file using  in-process eager execution mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import temporian as tp\n",
    "\n",
    "# Our processing function\n",
    "def my_processing(data):\n",
    "    # Re-index the events according to \"a\",\n",
    "    # and apply a 4 time-unit moving average.\n",
    "   \treturn data.add_index(\"a\").moving_sum(4)\n",
    "\n",
    "# Load the csv in memory into an EventSet \n",
    "input = tp.from_csv(\"input.csv\")\n",
    "\n",
    "# Apply the processing\n",
    "output = my_processing(input)\n",
    "\n",
    "# Save the results to a csv file\n",
    "tp.to_csv(output, path=\"output_in_process.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code applies the same processing and outputs the result in the `output_beam.csv` csv file using the Beam execution mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import temporian as tp\n",
    "import temporian.beam as tpb  # Import Temporian's Beam capabilities\n",
    "import apache_beam as beam\n",
    "\n",
    "# Same processing as before\n",
    "def my_processing(data):\n",
    "    return data.add_index(\"a\").moving_sum(4)\n",
    "\n",
    "# Create the input node i.e. the schema of our input\n",
    "input_node = tp.input_node([(\"a\", tp.str_), (\"b\", tp.float32)])\n",
    "# Create a Temporian graph. No computation is applied so far.\n",
    "output_node = my_processing(input_node)\n",
    "\n",
    "# Define a Beam pieline\n",
    "with beam.Pipeline() as pipeline:\n",
    "    output = (\n",
    "        pipeline\n",
    "        # Reads events from the csv file.\n",
    "        | tpb.from_csv(\"input.csv\", input_node.schema)\n",
    "        # Apply the processing \n",
    "        | tpb.run(input=input_node, output=output_node)\n",
    "        # Save the results to a csv file.\n",
    "        # Note: shard_name_template=\"\" outputs the results in a single csv file.\n",
    "        | tpb.to_csv(\"output_beam.csv\", output_node.schema, shard_name_template=\"\")\n",
    "    )\n",
    "\n",
    "    # Execute the pipeline\n",
    "    pipeline.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we check the content of the output csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat output_in_process.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat output_beam.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both files contain the same events. However, notice that events are not stored in the same order: In Temporian event order is not constrained, and different implementations and backend are free to change it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input and output data\n",
    "\n",
    "**Info:** Apache Beam represents data using PCollections. A PCollection is essentially a distributed and homogeneous collection of Python values. For example, a PCollection can contain strings, lists of strings, or more complex objects.\n",
    "\n",
    "You can import and export data from files using `temporian.beam.to_<format>` and `temporian.beam.from_<format>` functions, or you can write your own exporter on top of the [Beam IO connectors](https://beam.apache.org/documentation/io/connectors/).\n",
    "\n",
    "Here are some examples of data IO functions:\n",
    "\n",
    "- `temporian.beam.{from|to}_csv`: Import and export data from CSV files.\n",
    "- `temporian.beam.{from|to}_tensorflow_record`: Import and export data from TensorFlow Record files.\n",
    "\n",
    "The method `temporian.beam.to_event_set` and `temporian.beam.to_dict` can respectively convert dictionaries of values from and to Temporian Beam EventSets. For example:\n",
    "\n",
    "```python\n",
    "| ... # Generate dictionaries of values.\n",
    "| tpb.to_event_set(input_node.schema)\n",
    "| tpb.run(input=input_node, output=output_node)\n",
    "| tpb.to_dict(output_node.schema)\n",
    "| ... # Do something with the results\n",
    "```\n",
    "\n",
    "Some IO functions such as `to_event_set`, `to_dict` and `temporian.beam.{from|to}_tensorflow_record` support different formats to represent events and are controlled by the `format` attribute.\n",
    "\n",
    "- `format=\"single_events\"`: Each event is a dictionary of feature/index key to value. Each event is a different PCollection item.\n",
    "- `format=\"grouped_by_index\"` (default): Events with the same index are grouped in the same PCollection. Each item of this PCollection is a dictionary of feature/index key to value. Index values are represented as python primitives (e.g. int) and features are represented as Numpy arrays.\n",
    "\n",
    "The next code shows how to feed three events, with two features “a” and “b” and one index “c”, fed into Temporian with both the `format=\"single_events\"` and `format=\"grouped_by_index\"` format:\n",
    "\n",
    "```python\n",
    "# With format=\"single_events\"\n",
    "| pipeline\n",
    "| beam.Create([\n",
    "    # Index and features always have one value\n",
    "    {\"timestamp\": 1., \"a\": 4., \"b\": b\"X\", \"c\": 10},\n",
    "    {\"timestamp\": 2., \"a\": 5., \"b\": b\"Y\", \"c\": 10},\n",
    "    {\"timestamp\": 3., \"a\": 6., \"b\": b\"Z\", \"c\": 11},\n",
    "    ])\n",
    "| tpb.to_event_set(input_node.schema, format=\"single_events\")\n",
    "| tpb.run(...)\n",
    "| ...\n",
    "\n",
    "# With format=\"grouped_by_index\" (recommended)\n",
    "| pipeline\n",
    "| beam.Create([\n",
    "    {\"timestamp\": np.array([1., 2.], dtype=np.float64),\n",
    "     # \"a\" is a feature, so theree is one value per timestamp.\n",
    "    \"a\": np.array([4., 5,], dtype=np.float32),\n",
    "    \"b\": np.array([b\"X\", b\"Y\"], dtype=np.bytes_),\n",
    "    \"c\": 10}, # \"c\" is an index, so there is only one value.\n",
    "    \n",
    "    {\"timestamp\": np.array([3.], dtype=np.float64),\n",
    "    \"a\": np.array([6.,], dtype=np.float32),\n",
    "    \"b\": np.array([b\"Z\"], dtype=np.bytes_),\n",
    "    \"c\": 11}\n",
    "])\n",
    "| tpb.to_event_set(input_node.schema, format=\"grouped_by_index\")\n",
    "| tpb.run(...)\n",
    "| ...\n",
    "```\n",
    "\n",
    "**Warning:** Temporian dtype is not permissive. For example, a tp.float32 feature can only consume a numpy array of np.float32. Feeding a list or a numpy array of float64 will fail.\n",
    "\n",
    "Note `indexedEvents` is significantly faster than `singleEvents` format. When possible, use the `indexedEvents` format.\n",
    "\n",
    "Using `tpb.to_event_set` and `tpb.to_dict`, you can input and export data from other Beam IO connectors. The next example shows how to import events stored in TF.Records into Temporian.\n",
    "\n",
    "**Note:** This is just an example, TF.Records are natively supported by Temporian with `tpb.from_tensorflow_record`.\n",
    "\n",
    "```python\n",
    "def _parse_single_event_tf_example(example: example_pb2.Example) -> Dict[str, Union[float, int, bytes]]:\n",
    "    \"\"\"Converts a TF.Example proto into a dictionary of values.\"\"\"\n",
    "    \n",
    "    dict_example = {}\n",
    "    for k, v in example.features.feature.items():\n",
    "        if v.HasField(\"int64_list\"):\n",
    "            dict_example[k] = v.int64_list.value[0]\n",
    "        elif v.HasField(\"float_list\"):\n",
    "            dict_example[k] = v.float_list.value[0]\n",
    "        elif v.HasField(\"bytes_list\"):\n",
    "            dict_example[k] = v.bytes_list.value[0]\n",
    "        else:\n",
    "            raise ValueError(\"Bad feature\")\n",
    "    return dict_example\n",
    "\n",
    "\n",
    "| beam.io.tfrecordio.ReadFromTFRecord(\n",
    "    file_pattern=input,\n",
    "    coder=beam.coders.ProtoCoder(example_pb2.Example),\n",
    "    compression_type=beam.io.filesystem.CompressionTypes.GZIP)\n",
    "| beam.Map(_parse_single_event_tf_example)\n",
    "| tpb.to_event_set(input_node.schema, format=\"single_events\")\n",
    "| tpb.run(...)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed-up pipeline\n",
    "\n",
    "Following are some considerations to create fast Beam+Temporian pipelines on large datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider in-process execution mode\n",
    "\n",
    "The in-process execution mode runs on a single machine. If your dataset fits on a single machine, it is likely that in-process execution with the `@tp.compiled` or graph mode will be faster than the Beam execution mode.\n",
    "\n",
    "You can see the memory usage of an EventSet when printing it (e.g., `print(evset)`) or with the `evset.memory_usage()` method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide dataset into multiple files\n",
    "\n",
    "Ensure that any large dataset is divided into multiple files. As a rule of thumb, (rule 1) files should not be smaller than 10MB and (rule 2) there should be at least 10 files for each worker (unless rule 1 prevents it).\n",
    "\n",
    "For instance, if your dataset contains 100GB of data and you have 100 workers, the input dataset should be divided into 1000 files of size 100MB.\n",
    "\n",
    "Most Beam sharded output operations will automatically determine the number of output shards."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and export events using the indexedEvents format\n",
    "\n",
    "Prefer the `tpb.UserEventSetFormat.indexedEvents` event format to the `tpb.UserEventSetFormat.singleEvent` format. The format is specified as an argument of the import `tpb.to_event_set` and export `tpb.to_dict` functions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoid single index values with large amount of events\n",
    "\n",
    "**Note:** Temporian indexes are currently implemented using Beam keys. This means that all the values of a given feature and a given index key must be able to fit into the memory of a single worker.\n",
    "\n",
    "When intermediate values of a computation depend on index values with a large number of events, consider the following recommendations:\n",
    "\n",
    "Add indexes before removing them. For instance, prefer `x.add_index(“a”).drop_index(“b”)` over `x.drop_index(“b”).add_index(“a”)`.\n",
    "\n",
    "Apply aggregations before removing indexes. For instance, prefer `x.moving_sum(5).drop_index(“a”).moving_sum(tp.duration.shortest)` over `x.drop_index(a).moving_sum(5)`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import indexed EventSet\n",
    "\n",
    "Note: This rule is a special case of the “Avoid single index values with large amounts of events” and “Import and export events using the indexedEvents format” rules described above.\n",
    "\n",
    "When possible, your pipeline should consume and return indexed EventSets instead of creating those indexes during the computation.\n",
    "\n",
    "The following two examples return the same result. However, the first example can be significantly faster.\n",
    "\n",
    "**Example 1:** Load an indexed EventSet and then apply some transformations.\n",
    "\n",
    "```python\n",
    "input_node = tp.input_node(\n",
    "features=[(\"f\", tp.int32)],\n",
    "indexes=[(\"x\", tp.str_)])\n",
    "output_node = input_node.moving_sum(5)\n",
    "\n",
    "...\n",
    "| tpb.from_csv(\"input.csv\", input_node.schema)\n",
    "| tpb.run(input=input_node, output=output_node)\n",
    "...\n",
    "```\n",
    "\n",
    "**Example 2:** Load a non-indexed EventSet, add an index with the `add_index` operator, and apply the same transformations.\n",
    "\n",
    "```python\n",
    "input_node = tp.input_node(features=[(\"f\", tp.int32), (\"x\", tp.str_)])\n",
    "output_node = input_node.add_index(\"x\").moving_sum(5)\n",
    "\n",
    "...\n",
    "| tpb.from_csv(\"input.csv\", input_node.schema)\n",
    "| tpb.run(input=input_node, output=output_node)\n",
    "...\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
