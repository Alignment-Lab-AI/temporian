{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98a0068b-af99-47d9-a6b2-9b36debd2880",
   "metadata": {},
   "source": [
    "# M5 Competition\n",
    "\n",
    "## Introduction\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/temporian/blob/main/docs/src/tutorials/m5_competition.ipynb)\n",
    "\n",
    "The M5 Competition, held in 2020, was part of the prestigious Makridakis Forecasting Competitions. The goal of this competition was to accurately forecast the sales of 3,000 individual items across 10 Walmart stores for the next 28 days. The winning strategy involved transforming and augmenting the raw multivariate time series data into a tabular dataset, which was then used to train a collection of Gradient Boosted Trees models.\n",
    "\n",
    "See https://www.kaggle.com/competitions/m5-forecasting-accuracy/overview.\n",
    "\n",
    "In this notebook, we show how Temporian can replicate such data preprocessing and feature engineering with low effort. More specifically, we will replicate, with much less code, the preprocessing demonstrated at the [Khipu 2023: A Hands-On Forecasting Guide](https://colab.sandbox.google.com/drive/1nJ7GP0kayoCt6J-QkhyJw8jvr5uLnLcy), which was done with Pandas.\n",
    "\n",
    "The notebook is divided into three parts:\n",
    "\n",
    "- Loading the dataset and initial cleaning using Pandas.\n",
    "- Temporal feature engineering using Temporian.\n",
    "- Training and evaluation of a model using TensorFlow Decision Forests.\n",
    "\n",
    "The following preprocessing operations will be applied using Temporian:\n",
    "\n",
    "- **label**: Compute forecasting labels i.e. future leak of sales data.\n",
    "- **lagged sales**: Give access to previous sales data.\n",
    "- **moving statistics**: Moving average, sum and standard deviation of the sales over the last 7, 14, 28 and 64 days.\n",
    "- **calendar features**: Extract discriminative calendar features such as day of the week, day of the month, etc.\n",
    "- **aggregated sales**: Aggregate sales per department. Each item has access to the sales of its corresponding department.\n",
    "- **special events**: Holidays, sales, and other special events.\n",
    "- **train / test split**: Split the data between a training and testing dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93035f73-8297-4a5f-8a47-1af913f002c8",
   "metadata": {},
   "source": [
    "## Install and import dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc838e-e565-4d86-9eae-8a686ad6b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install temporian -q\n",
    "\n",
    "# The Bokeh package is used by Temporian to create interactive plots.\n",
    "# If if Bokeh is not installed, Temporian plots are static.\n",
    "%pip install bokeh -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d9e3a-b105-48e1-a47d-295312604774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import temporian as tp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34b3515b-9368-4ff5-80bf-181be1fe6593",
   "metadata": {},
   "source": [
    "## Download the raw data\n",
    "\n",
    "The M5 dataset is a collection of csv files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb18926-5aec-4f36-8b04-008488eade04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to download the raw M5 dataset and to export the result.\n",
    "work_directory = \"tmp/temporian_m5\"\n",
    "os.makedirs(work_directory, exist_ok=True)\n",
    "\n",
    "# Download the M5 dataset\n",
    "raw_data_zip = os.path.join(work_directory, \"raw.zip\")\n",
    "if not os.path.exists(raw_data_zip):\n",
    "    url = \"https://docs.google.com/uc?export=download&id=1NYHXmgrcXg50zR4CVWPPntHx9vvU5jbM&confirm=t\"\n",
    "    urllib.request.urlretrieve(url, raw_data_zip)\n",
    "\n",
    "# Extract the M5 dataset\n",
    "raw_data_dir = os.path.join(work_directory, \"raw\")\n",
    "if not os.path.exists(raw_data_dir):\n",
    "    with zipfile.ZipFile(raw_data_zip, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(raw_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105c5f4a-0ab1-4de1-b46b-5b80254a5ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh tmp/temporian_m5/raw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5537c78f-e441-4e55-8e5b-b6fe06bf281c",
   "metadata": {},
   "source": [
    "## Load raw dataset\n",
    "\n",
    "The dataset is a set of csv files. We are using Pandas to load it and do some initial non-temporal preparations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e144ebe2-dc46-46e3-8d89-67355aed7000",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = lambda x: os.path.join(raw_data_dir, x)\n",
    "\n",
    "raw_sales = pd.read_csv(raw_path(\"sales_train_evaluation.csv\"))\n",
    "raw_sell_prices = pd.read_csv(raw_path(\"sell_prices.csv\"))\n",
    "raw_calendar = pd.read_csv(raw_path(\"calendar.csv\"))\n",
    "\n",
    "# Print the name, type and memory usage of the raw data columns.\n",
    "print(\"==========\\nraw_sales\\n\")\n",
    "raw_sales.info()\n",
    "print(\"==========\\nraw_sell_prices\\n\")\n",
    "raw_sell_prices.info()\n",
    "print(\"==========\\nraw_calendar_raw\\n\")\n",
    "raw_calendar.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb1c2481-84e0-4876-91e3-8fee0e1b11ce",
   "metadata": {},
   "source": [
    "### Downsample the dataset\n",
    "\n",
    "During development, we restrict the dataset to a subset of items for fast iteration.\n",
    "Once you are happy with the result, we can use the full dataset with `downsample_dataset=False`.\n",
    "\n",
    "Each product will result in ~1900 examples in the final tabular dataset. For example, using\n",
    "a sample of 100 products will train a model on 190.000 examples which is more than enough for a quick demonstration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be833bab-ef50-4809-8341-23938b1279f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample_dataset = True\n",
    "\n",
    "if downsample_dataset:\n",
    "    # The items with the most sales.\n",
    "    selected_items_names = ['FOODS_3_090', 'FOODS_3_586', 'FOODS_3_252', 'FOODS_3_555',\n",
    "                            'FOODS_3_587', 'FOODS_3_714', 'FOODS_3_694', 'FOODS_3_226',\n",
    "                            #'FOODS_3_202', 'FOODS_3_120', 'FOODS_3_723', 'FOODS_3_635',\n",
    "                            #'FOODS_3_808', 'FOODS_3_377', 'FOODS_3_541', 'FOODS_3_080',\n",
    "                            #'FOODS_3_318', 'FOODS_2_360', 'FOODS_3_681', 'FOODS_3_234',\n",
    "                           ]\n",
    "    \n",
    "    sampled_raw_sales = raw_sales[raw_sales[\"item_id\"].isin(selected_items_names)]\n",
    "    print(\"Number of selected items:\", len(sampled_raw_sales))\n",
    "\n",
    "    raw_sales = sampled_raw_sales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1886f09-6ae1-4791-a0d6-b4d17387600c",
   "metadata": {},
   "source": [
    "### Normalize dataset\n",
    "\n",
    "Each of the three tables `sales_raw`, `sell_prices_raw` and `calendar_raw` contains different information: `sales_raw` contains the daily sales of each product in each store, `sell_prices_raw` contains selling prices, and `calendar_raw` contains calendar events such as holidays.\n",
    "\n",
    "Each table represents temporal data in a different way. For example, timestamps are stored by columns in `sales_raw` and by rows in `sell_prices_raw` and `calendar_raw`. The three tables also use a different system to represent dates.\n",
    "\n",
    "In Temporian, the data is always expressed as an **EventSet**. Let's normalize and convert the raw tables to EventSets. EventSets can be built manually with `tp.event_set()` or they can be imported from Pandas DataFrames with `tp.from_pandas()`. In this case, timestamps need to be organized by rows.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00c50ff7-02b1-459b-a5cd-7781580d3595",
   "metadata": {},
   "source": [
    "#### Normalize `raw_sales`\n",
    "\n",
    "Before:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df78d002-e33d-4b02-aec0-d5deca909805",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sales.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3081b911-48df-4a39-8678-171a1dde8149",
   "metadata": {},
   "source": [
    "Each daily sales value is stored in a different column. Instead, we want\n",
    "for each value (which in Temporian we refer to as an event) to be in a different row.\n",
    "\n",
    "For example, the record:\n",
    "\n",
    "```\n",
    "id,item_id,dept_id,d_1,d_2,d_3,...,d_n\n",
    "```\n",
    "\n",
    "will be converted to the following records:\n",
    "\n",
    "```\n",
    "id,item_id,dept_id,day,d_1\n",
    "id,item_id,dept_id,day,d_2\n",
    "...\n",
    "id,item_id,dept_id,day,d_n\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b55d1f4-cf72-4f62-92ee-c988b94fc01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sales = pd.melt(\n",
    "    raw_sales,\n",
    "    var_name=\"day\",\n",
    "    value_name=\"sales\",\n",
    "    id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n",
    ")\n",
    "cleaned_sales[\"day\"] = cleaned_sales[\"day\"].apply(lambda x: int(x[2:]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "594e5f2c-2b7a-4aeb-8165-8e9527929690",
   "metadata": {},
   "source": [
    "The timestamps are expressed in number of days since `29/1/2011`.\n",
    "Let's convert them into python datetimes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe44b50-afbc-40ab-bd64-7ec231baf123",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_date = datetime(2011, 1, 29, tzinfo=timezone.utc)\n",
    "cleaned_sales[\"timestamp\"] = cleaned_sales[\"day\"].apply(\n",
    "    lambda x: (origin_date + timedelta(days=x - 1))\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33aafd0d-79d8-41bc-accb-375307ab7660",
   "metadata": {},
   "source": [
    "Finally, we can remove the fields we won't use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4688a3e7-adff-449c-a5dc-0e052b52c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del cleaned_sales[\"id\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04b4620c-7312-409e-80df-d2acee173e6d",
   "metadata": {},
   "source": [
    "After:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa03718-4050-400b-b5cb-f8402a95766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sales.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8deb0ffd-0380-4540-9596-44bdde8b50e1",
   "metadata": {},
   "source": [
    "Let's now convert `cleaned_sales` to a Temporian EventSet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801c3ab0-de0b-400d-9416-a74917c507dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_evset = tp.from_pandas(\n",
    "    cleaned_sales,\n",
    "    indexes=[\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n",
    ")\n",
    "\n",
    "sales_evset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40dd83fa-742f-41d1-b23b-cfab43f4ad3d",
   "metadata": {},
   "source": [
    "Let's plot the sales of the first two products.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb44fde1-d2b4-4ff8-b0c2-4367ee95e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_evset.plot(max_num_plots=4, interactive=True, width_px=500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "502fe9ef-b650-439d-99c1-05c7d50c7dd6",
   "metadata": {},
   "source": [
    "#### Normalize `raw_calendar`\n",
    "\n",
    "In `raw_calendar`, timestamps are expressed as ISO 8601 string e.g., `2011-01-29`. Let's convert them into python datetimes.\n",
    "\n",
    "Before:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9adcbde-35b6-4a61-a12d-4e0bd2ae62fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_calendar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4530d2e8-2c12-4a05-b3cc-ef2eed90fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_calendar = raw_calendar.copy()\n",
    "\n",
    "# Temporian and TensorFlow Decision Forests (after) treat NaN values as \"missing\".\n",
    "# In this dataset, a NaN means that there is not calendar event on this day.\n",
    "cleaned_calendar.fillna(\"no_event\", inplace=True)\n",
    "\n",
    "cleaned_calendar[\"timestamp\"] = cleaned_calendar[\"date\"].apply(\n",
    "    lambda x: datetime.strptime(x, \"%Y-%m-%d\")\n",
    ")\n",
    "\n",
    "# We keep the mapping wm_yr_wk -> timestamp to clean \"raw_sell_prices\" in the next section.\n",
    "wm_yr_wk_map = cleaned_calendar[[\"weekday\", \"timestamp\", \"wm_yr_wk\"]]\n",
    "\n",
    "del cleaned_calendar[\"date\"]\n",
    "del cleaned_calendar[\"wm_yr_wk\"]\n",
    "del cleaned_calendar[\"d\"]\n",
    "del cleaned_calendar[\"weekday\"]\n",
    "del cleaned_calendar[\"wday\"]\n",
    "del cleaned_calendar[\"month\"]\n",
    "del cleaned_calendar[\"year\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39ef4a73-8f45-4795-a099-d82bf946f054",
   "metadata": {},
   "source": [
    "After:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b58a95-86e1-4eae-87b7-fe18f95a2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_calendar.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98db7450-143b-402a-a1c7-57f8e01574f0",
   "metadata": {},
   "source": [
    "Same as before, we can convert the calendar data into a Temporian EventSet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e667774f-c040-4006-b214-ae841d9e6d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_evset = tp.from_pandas(cleaned_calendar)\n",
    "\n",
    "calendar_evset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3eb96d26-34fa-4eff-91c7-37b8329d40ef",
   "metadata": {},
   "source": [
    "#### Normalize `raw_sell_prices`\n",
    "\n",
    "Before:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce4218a-594b-41d6-b7de-418a9414b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sell_prices.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd139685-4a54-4fae-a32e-0ebf079ff1cc",
   "metadata": {},
   "source": [
    "In `sell_prices`, timestamps as expressed with a special date format called `wm_yr_wk`.\n",
    "We use the `calendar` data to find the mapping between `wm_yr_wk` and classical python datetimes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6279d6dc-06f7-4c06-b329-40d58ec69144",
   "metadata": {},
   "outputs": [],
   "source": [
    "wm_yr_wk_to_date = wm_yr_wk_map[wm_yr_wk_map[\"weekday\"] == \"Saturday\"][\n",
    "    [\"timestamp\", \"wm_yr_wk\"]\n",
    "]\n",
    "\n",
    "map_wm_yr_wk_to_date = {}\n",
    "for _, row in wm_yr_wk_to_date.iterrows():\n",
    "    map_wm_yr_wk_to_date[row[\"wm_yr_wk\"]] = row[\"timestamp\"]\n",
    "\n",
    "cleaned_sell_prices = raw_sell_prices.copy()\n",
    "cleaned_sell_prices[\"timestamp\"] = cleaned_sell_prices[\"wm_yr_wk\"].apply(\n",
    "    lambda x: map_wm_yr_wk_to_date[x]\n",
    ")\n",
    "\n",
    "del cleaned_sell_prices[\"wm_yr_wk\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92fb02e3-5b37-4b79-895c-0cbfa2b9a082",
   "metadata": {},
   "source": [
    "After:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dc338e-6d84-4be6-aac2-416d1acabd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sell_prices.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38a23ace-df0f-49f2-9f65-20febf5ce33f",
   "metadata": {},
   "source": [
    "Same as before, we can convert the calendar data into a Temporian EventSet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa7a3ed-f033-4e5c-b41f-6cf5fa71bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices_evset = tp.from_pandas(\n",
    "    cleaned_sell_prices, indexes=[\"store_id\", \"item_id\"]\n",
    ")\n",
    "\n",
    "sell_prices_evset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ee43d5a-24ea-445e-b14e-05cc7a316fda",
   "metadata": {},
   "source": [
    "Now that our data is in the Temporian format, we can delete the Pandas DataFrames to recover some memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c824aa-7aa4-4ce6-a0cb-17a6d69ef084",
   "metadata": {},
   "outputs": [],
   "source": [
    "del raw_sales\n",
    "del raw_sell_prices\n",
    "del raw_calendar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b312c55-5673-49a6-8733-13c376b2c1e7",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "The EventSets `sales_evset`, `calendar_evset` and `sell_prices_evset` are indexed by product.\n",
    "To illustrate each of the newly computed features, we will plot them on a single product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0447be99-3c1c-4fd5-9b25-47a8f3fc9eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a product.\n",
    "selected_index = sales_evset.get_arbitrary_index_key()\n",
    "\n",
    "print(\"Selected product (selected_index):\", selected_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c721f1f9-7885-49c3-bf28-410aeec4fc13",
   "metadata": {},
   "source": [
    "The following arguments will be used for `tp.plot()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f6d0e-4555-4835-8e84-712a6e892e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_options = {\n",
    "    # Only plot the selected product.\n",
    "    \"indexes\": selected_index,\n",
    "    # Make the plot interactive.\n",
    "    \"interactive\": True,\n",
    "    # Make sure the toolbar is visible\n",
    "    \"width_px\": 500,\n",
    "    # Only plot the data for the year 2015.\n",
    "    \"min_time\": datetime(2015, 1, 1),\n",
    "    \"max_time\": datetime(2015, 12, 31),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68aef8af-c9e7-4faa-b50c-b5d44abcae29",
   "metadata": {},
   "source": [
    "`all_features` will contain all of the generated features, and we'll use the prefix `f_` for all their names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cec8ef-62a0-4d99-b46c-a217ae012f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = [\n",
    "    # The raw sales is the first feature.\n",
    "    sales_evset[\"sales\"].prefix(\"f_\"),\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f69e7c80-c30f-499b-8c08-b5b4876b0d9d",
   "metadata": {},
   "source": [
    "### Label\n",
    "\n",
    "The labels at time `t` are the sales at time `t+1..28`. This is computed with the `EventSet.leak()` operator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e74746-76f7-4992-a9c6-2d68d06bbaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list: List[tp.EventSet] = []\n",
    "\n",
    "# Prediction horizon. In the M5 competition, participants should forecast the next 28 days.\n",
    "# You can predict less days to make the training section of this notebook run faster.\n",
    "horizons = list(range(1, 28))\n",
    "\n",
    "# For each of the horizons.\n",
    "for horizon in horizons:\n",
    "    # Leak the sales in the past.\n",
    "    x = sales_evset[\"sales\"].leak(tp.duration.days(horizon))\n",
    "\n",
    "    # Resample the label to the sales data.\n",
    "    x = x.resample(sales_evset)\n",
    "\n",
    "    # Give a name to the label for book-keeping.\n",
    "    x = x.rename(f\"label_horizon_{horizon}_days\")\n",
    "    label_list.append(x)\n",
    "\n",
    "labels = tp.glue(*label_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fd31e0b-3c98-4d03-b3d7-6b8538b01bfc",
   "metadata": {},
   "source": [
    "Let's see what the labels look like. Can you see how they shift more and more to the left while the horizon increases?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa783ac-4c30-4d0e-9dfa-2dd05c6bd696",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.plot(**plot_options, max_num_plots=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "552e640b-029a-4d98-9546-52f1f903af54",
   "metadata": {},
   "source": [
    "### Past observations i.e. lagged sales\n",
    "\n",
    "The model will have access to the daily sales in the last 3 days, as well as in the same day of the last and second-to-last weeks.\n",
    "\n",
    "This is similar to computing the labels, except that the \"shift\" is in the opposite direction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609439b4-a424-4fad-8291-c94cc4de668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged_sales_list: List[tp.EventSet] = []\n",
    "\n",
    "for horizon in [1, 2, 3, 7, 14]:\n",
    "    x = sales_evset[\"sales\"].lag(tp.duration.days(horizon))\n",
    "    x = x.resample(sales_evset)\n",
    "    x = x.rename(f\"f_sales_lag_{horizon}_d\")\n",
    "    lagged_sales_list.append(x)\n",
    "\n",
    "feature_lagged_sales = tp.glue(*lagged_sales_list)\n",
    "\n",
    "all_features.append(feature_lagged_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83888d9a-2e94-4d62-ae30-4815896ad4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_lagged_sales.plot(**plot_options, max_num_plots=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f9ac1d2-58a5-43a2-932d-8a0d0c4634cb",
   "metadata": {},
   "source": [
    "### Moving statistics\n",
    "\n",
    "Moving statistics can help the model identify global trends in the data (moving average) or periods of high or low volatility (moving standard deviation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995ea19e-d39a-4f94-a010-6cd130e623c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_stats_list: List[tp.EventSet] = []\n",
    "\n",
    "float_sales = sales_evset[\"sales\"].cast(tp.float32)\n",
    "for win_day in [7, 14, 28, 84]:\n",
    "    win = tp.duration.days(win_day)\n",
    "\n",
    "    x = float_sales.simple_moving_average(win).prefix(\n",
    "        f\"f_sma_{win_day}_\"\n",
    "    )\n",
    "    moving_stats_list.append(x)\n",
    "\n",
    "    x = float_sales.moving_standard_deviation(win).prefix(\n",
    "        f\"f_sd_{win_day}_\"\n",
    "    )\n",
    "    moving_stats_list.append(x)\n",
    "\n",
    "    x = float_sales.moving_sum(win).prefix(f\"f_sum_{win_day}_\")\n",
    "    moving_stats_list.append(x)\n",
    "\n",
    "feature_moving_stats = tp.glue(*moving_stats_list)\n",
    "\n",
    "all_features.append(feature_moving_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70161a5f-9e63-4e01-ab98-c5e4a96f59e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_moving_stats.plot(**plot_options, max_num_plots=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed0d8bd0-a243-44d8-8d89-34ac9166d18e",
   "metadata": {},
   "source": [
    "### Calendar features\n",
    "\n",
    "The model will not be able to use dates directly. Instead, we transform the date into numerical features that can be consumed by a model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e67fff6-d75a-4d04-991c-649e0c5374a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_list: List[tp.EventSet] = []\n",
    "calendar_list.append(sales_evset.calendar_day_of_month())\n",
    "calendar_list.append(sales_evset.calendar_day_of_week())\n",
    "calendar_list.append(sales_evset.calendar_month())\n",
    "feature_calendar = tp.glue(*calendar_list).prefix(\"f_\")\n",
    "\n",
    "all_features.append(feature_calendar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63294d29-6e78-4e01-b098-fcb704065ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_calendar.plot(**plot_options, max_num_plots=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7ba5b0c-9d86-4b08-825a-6bb565de23ac",
   "metadata": {},
   "source": [
    "### Sales aggregated per department\n",
    "\n",
    "When predicting the sales of a product, the sales of the other products in the same department can help forecasting.\n",
    "\n",
    "Let's first group all the sales per department in each store, by removing the `item_id` feature from our EventSet's index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a0b18c-0eb8-4ebc-85d1-a928ac7355d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `sales_per_dept` contains all the sales indexed by department / category / store / state.\n",
    "# Aggregating the sales at other levels (e.g., by store) could also be useful.\n",
    "sales_per_dept = sales_evset.drop_index(\"item_id\", keep=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c75206dc-cb4b-4225-bd36-19ceafdfc00b",
   "metadata": {},
   "source": [
    "`sales_per_dept` still contains the sales of each item: each day, `sales_per_dept` contains one record for each item.\n",
    "We need to aggregate those sales together on each day."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8304c6ff-4f49-41f4-ac32-a949129938b9",
   "metadata": {},
   "source": [
    "We compute the 28 days moving sum of sales for this department. To improve the quality of the final model, we could additionally use other time windows e.g. 1, 3, 7, 28.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9f3d61-3aa3-4a15-9065-9da27b747107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a daily sampling.\n",
    "sampling_once_a_day = sales_per_dept.unique_timestamps()\n",
    "\n",
    "# Cumulative 28 days sum of sales, per department.\n",
    "sum28_sales_per_dept = sales_per_dept[\"sales\"].moving_sum(\n",
    "        window_length=tp.duration.days(28),\n",
    "        sampling=sampling_once_a_day\n",
    "    )\n",
    "\n",
    "# Give it a name for book-keeping.\n",
    "sum28_sales_per_dept = sum28_sales_per_dept.prefix(\"f_sum28_per_dep_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6bd2ff-8383-43e1-9202-e42fcec713bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum28_sales_per_dept.plot(max_num_plots=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffb3b2b5-18f5-4187-b72f-e8848e0a9f7b",
   "metadata": {},
   "source": [
    "The sales of each department is then _propagated_ back to each corresponding product.\n",
    "\n",
    "Note that two products from the same department will receive the same features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6b8394-de68-4ce6-898b-02f5d2baed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum28_sales_per_dept = sum28_sales_per_dept.propagate(sales_evset, resample=True)\n",
    "\n",
    "all_features.append(sum28_sales_per_dept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dfa773-1cff-47c9-babc-a003bd2c9315",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum28_sales_per_dept.plot(max_num_plots=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "210c3230-06b8-4f13-8a71-ec68ff667cf9",
   "metadata": {},
   "source": [
    "### Special events\n",
    "\n",
    "Special events such as holidays and sale periods can impact sales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f08bd5-5c88-424e-9db4-42e241fb51bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagate the global special event to each product.\n",
    "special_event_per_product = calendar_evset.propagate(sales_evset, resample=True)\n",
    "\n",
    "# Since special events are known in advance, we can leak future special events in the past.\n",
    "#\n",
    "# For example, on 22/12/2016, the model can know that it will be christmas in 2 days.\n",
    "special_events_list: List[tp.EventSet] = []\n",
    "special_events_list.append(special_event_per_product.prefix(\"f_\"))\n",
    "\n",
    "for leak in [5]:\n",
    "    x = special_event_per_product.leak(tp.duration.days(leak))\n",
    "    x = x.resample(sales_evset)\n",
    "    x = x.prefix(f\"f_leak{leak}_\")\n",
    "    special_events_list.append(x)\n",
    "\n",
    "# To make the model more powerful, we could also apply some moving statistics such\n",
    "# as moving-maximum to create features such as: is it christmas in the following N days.\n",
    "\n",
    "feature_special_events = tp.glue(*special_events_list)\n",
    "\n",
    "all_features.append(feature_special_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af507fc1-a9ea-45c3-86ea-d37661847a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_special_events.plot(**plot_options, max_num_plots=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3a6532f-e01f-477e-80e2-90f0a6cd75eb",
   "metadata": {},
   "source": [
    "### Review all the features\n",
    "\n",
    "`all_features` contains all the features we want to feed to our model.\n",
    "`tabular` is the data that will be used to train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed92623-3363-4313-9e9b-5e803a4202f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular = tp.glue(*all_features, sales_evset[\"day\"], labels)\n",
    "\n",
    "tabular.schema"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a7eb607-f492-4b3a-96b7-44d5fed6a1af",
   "metadata": {},
   "source": [
    "## Generate training and testing datasets\n",
    "\n",
    "Compute all the new features and labels on the dataset.\n",
    "\n",
    "The test dataset is the single day 1914. In this day, the model makes 28 predictions, one for each horizon, for each of the products.\n",
    "\n",
    "For training, we use the data prior to the test day. We also remove the first 30 days as those days don't have enough data to compute good statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0e672-7a16-42df-82cd-994132ee0fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_test = tabular.filter(tp.equal_scalar(tabular[\"day\"], 1914))\n",
    "tabular_train = tabular.filter(\n",
    "    (tabular[\"day\"] >= 30) & (tabular[\"day\"] < 1914)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acd1995c-141f-4703-81f0-fa7bd2839079",
   "metadata": {},
   "source": [
    "Let's compute the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b642e-6750-44b6-b4eb-a26b2bcdcf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tp.to_pandas(tabular_test)\n",
    "\n",
    "test_dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de0d77b6-216f-4f54-a173-b2f600fa2d2d",
   "metadata": {},
   "source": [
    "Let's compute the train dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799a0b07-50a8-4af3-8ed4-9131b5a102ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tp.to_pandas(tabular_train)\n",
    "\n",
    "train_dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b90b6f49-82ca-4f36-924b-28d61a77210f",
   "metadata": {},
   "source": [
    "We can convert those EventSets to Pandas DataFrames and save them to csv files for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c936ac9a-ef09-4243-bf69-4b9ff73a2efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.to_csv(os.path.join(work_directory, \"test.csv\"), index=False)\n",
    "train_dataset.to_csv(os.path.join(work_directory, \"train.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f231d2-499e-4a77-b635-21e469f1dce6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!head tmp/temporian_m5/train.csv -n 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6190ebbc-daba-4a58-8b0e-224f230c877d",
   "metadata": {},
   "source": [
    "### Training and evaluating a simple model\n",
    "\n",
    "Now that we have pre-processed data, we can train a model. We will train a collection of Gradient Boosted Trees using TensorFlow Decision Forests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d6d3b1-32d0-4d4c-b829-814277353ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow tensorflow_decision_forests -q\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_decision_forests as tfdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6666033-3849-4418-bfc8-5a791221e5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the labels and features.\n",
    "label_names = [x for x in test_dataset.columns if x.startswith(\"label_\")]\n",
    "feature_names = [x for x in test_dataset.columns if x.startswith(\"f_\")] + [\n",
    "    # The model also has access to the meta-data.\n",
    "    \"dept_id\",\n",
    "    \"cat_id\",\n",
    "    \"store_id\",\n",
    "    \"state_id\",\n",
    "] \n",
    "\n",
    "print(\"label_names:\", label_names)\n",
    "print(\"feature_names:\", feature_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de1f639f-c66e-439e-a5c9-871178f50720",
   "metadata": {},
   "source": [
    "The Pandas DataFrame can be converted into a TensorFlow dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a46a286-1825-49ec-a147-f539987da590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_pandas_to_tensorflow(df):\n",
    "    features = {k: df[k] for k in feature_names}\n",
    "    labels = {k: df[k] for k in label_names}\n",
    "    return tf.data.Dataset.from_tensor_slices((features, labels)).batch(100)\n",
    "\n",
    "\n",
    "tf_test_dataset = dataset_pandas_to_tensorflow(test_dataset)\n",
    "tf_train_dataset = dataset_pandas_to_tensorflow(train_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "328a6972-fd28-4c8d-b81b-d78481c29c67",
   "metadata": {},
   "source": [
    "We can train our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7580bf-bd81-4806-9dde-1465b0a6fb00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(work_directory, \"gbm.model\")\n",
    "if os.path.exists(model_path):\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "else:\n",
    "    model = tfdf.keras.GradientBoostedTreesModel(\n",
    "        multitask=[\n",
    "            tfdf.keras.MultiTaskItem(x, task=tfdf.keras.Task.REGRESSION)\n",
    "            for x in label_names\n",
    "        ],\n",
    "        num_trees=50,  # Increase the number of trees (e.g., 100) for a better model.\n",
    "        verbose=2, # Remove to make the cell less verbose.\n",
    "        num_threads=8,\n",
    "    )\n",
    "    \n",
    "    model.fit(tf_train_dataset)\n",
    "    model.save(model_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95da6100-af7c-4ad5-85ff-a5f2361d6bca",
   "metadata": {},
   "source": [
    "Let's generate the predictions on the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e985dba0-dc14-4341-b0c8-ad145fef7ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_predictions = model.predict(tf_test_dataset, verbose=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf450d44-4fb7-4de5-9611-e3fbe9c6c48a",
   "metadata": {},
   "source": [
    "Let's convert those TensorFlow predictions into a Temporian EventSet. This way we will be able to plot the predictions alongside the real sales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068ea997-52ae-40eb-b18e-a118a3d7b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_names = tabular_test.schema.index_names()\n",
    "\n",
    "raw_predicted_sales = defaultdict(list)\n",
    "timestamps = []\n",
    "\n",
    "# For each product\n",
    "for row_idx in range(len(test_dataset)):\n",
    "    original_timestamp = datetime.fromtimestamp(\n",
    "        test_dataset[\"timestamp\"][row_idx], tz=timezone.utc\n",
    "    )\n",
    "\n",
    "    # For each prediction horizon\n",
    "    for horizon, predictions in zip(horizons, tf_predictions.values()):\n",
    "        timestamps.append(original_timestamp + timedelta(days=horizon))\n",
    "\n",
    "        for k in index_names:\n",
    "            raw_predicted_sales[k].append(test_dataset[k][row_idx])\n",
    "\n",
    "        raw_predicted_sales[\"prediction_sales\"].append(predictions[row_idx, 0])\n",
    "\n",
    "# Convert the dictionary of values into an event set.\n",
    "prediction_evset = tp.event_set(\n",
    "    timestamps, raw_predicted_sales, indexes=index_names\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35c68a22-738b-41fd-8493-d1f359724e0a",
   "metadata": {},
   "source": [
    "Finally, we can plot the real sales and predicted sales.\n",
    "\n",
    "**Remember:** Update the variable `horizons` to change the number of days the model is predicting (14 by default)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e18ed-cf9c-4fcd-8373-b40706c03ba1",
   "metadata": {},
   "source": [
    "### Plot predictions\n",
    "\n",
    "Let's now show some predictions and compare them against the real sales. The sales before the test day 1914 are considered past sales, and then we compare the predictions against real sales after the test day.\n",
    "\n",
    "In this cell we'll just use the standard plots, and then we'll customize them to better compare predictions against ground-truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978fc1ff-7914-43df-af04-e5b054a9e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_sales_evset = sales_evset[\"sales\"].filter(sales_evset[\"day\"] >= 1914)\n",
    "\n",
    "past_sales_evset = sales_evset[\"sales\"].filter((sales_evset[\"day\"] <= 1914) & (sales_evset[\"day\"] >= 1914 - 40))\n",
    "\n",
    "# Rename and plot together\n",
    "real_sales_evset = real_sales_evset.rename(\"real_sales\")\n",
    "past_sales_evset = past_sales_evset.rename(\"past_sales\")\n",
    "prediction_evset = prediction_evset.rename(\"predicted_sales\")\n",
    "\n",
    "tp.plot(\n",
    "    [real_sales_evset, prediction_evset, past_sales_evset],\n",
    "    max_num_plots=3*2,\n",
    "    merge=True,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f145087c-4629-4639-b7fb-4d19531fbc8e",
   "metadata": {},
   "source": [
    "How does that model work? We can look at the variable importance to get an idea of what feature matters the most with `model.summary()`.\n",
    "\n",
    "Note that this model contains in fact one individual model for each horizon. In other words, this model contains 28 sub models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
