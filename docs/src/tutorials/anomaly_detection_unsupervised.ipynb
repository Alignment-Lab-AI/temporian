{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c6c457-fd5c-494c-add1-54c00798f34f",
   "metadata": {},
   "source": [
    "# Unsupervised anomaly detection with Temporian and scikit-learn\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/temporian/blob/last-release/docs/src/tutorials/anomaly_detection_unsupervised.ipynb)\n",
    "\n",
    "In this tutorial, we use the Temporian and YDF python libraries to detect anomalies in a multivariate time series dataset.\n",
    "\n",
    "Anomaly detection in time series, time sequences and other temporal data representations is critical in a variety of domains. For instance, it is used in manufacturing to detect equipment failure and production lines, in computer systems and by financial institutions to detect fraudulent activities, and in energy management to detect outages.\n",
    "\n",
    "We will use the Server Machine Dataset (SMD) dataset, published as part of the [OmniAnomaly](https://github.com/NetManAIOps/OmniAnomaly/) paper, which is available in CSV files in that same repository.\n",
    "\n",
    "This dataset contains aggregated resource usage metrics (e.g., CPU and RAM utilization, network traffic) from 28 computers in 3 data centers over a period of 5 weeks. The timestamps and feature names have been anonymized and normalized. Therefore, calendar feature engineering or the use of expert knowledge is not possible.\n",
    "\n",
    "The dataset will be loaded, feature engineered and converted into a tabular dataset using Temporian.\n",
    "Then, we will use an isolation forest model from the YDF library on this tabular data to detect anomalies.\n",
    "Finally, we will evaluate the quality of our detection on the ground truth anomalies, available as part of the testing dataset.\n",
    "\n",
    "We will first evaluate our detections both with AUC-[ROCs](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) (Area Under the Receiver operating Characteristic curve) and with [AMOCs](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2815453/) (Activity Monitoring Operating Characteristic). Unlike an ROC, an AMOC takes into account the *time to detection* or *forecasting horizon* which is often critical in temporal problems like ours. If you don't know about AMOCs yet, don't worry. We will explain it.\n",
    "\n",
    "Check out the [Supervised anomaly detection](https://temporian.readthedocs.io/en/stable/tutorials/anomaly_detection_supervised/) tutorial for a version of this notebook that trains a supervised model using the ground truth labels, which is less common in an anomaly detection setting, but can yield better performance if those are available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd78b58b-4d14-4306-bb45-276d1de60b11",
   "metadata": {},
   "source": [
    "## Installation and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d844ff1b-224d-4e36-b7d1-dae8ed4df263",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install temporian ydf scikit-learn -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe822a6c-048e-4f60-b8ec-be50d724726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ydf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import urllib.request\n",
    "\n",
    "import temporian as tp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d796dd5b-aeb8-4ce5-820c-a1a1fb39f5b1",
   "metadata": {},
   "source": [
    "## Download dataset\n",
    "\n",
    "The dataset is made of 3 groups of 8, 9, and 11 machines respectively, with names `\"machine-1-1\"`, ..., `\"machine-3-11\"`.\n",
    "Let's list the machine names, and download records locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e591261-9c5a-4b75-acc7-1dbe850f56d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "machines_per_group = [8, 9, 11]  # Full dataset\n",
    "# machines_per_group = [3, 3, 3]  # Subset of data\n",
    "\n",
    "machines = [f\"machine-{group}-{id}\" for group, machine in zip(range(1, 4), machines_per_group) for id in range(1, machine + 1)]\n",
    "print(f\"{len(machines)} machines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9edfc8c-d82b-46b2-a20e-6281383279b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"tmp/temporian_server_machine_dataset\")\n",
    "dataset_url = \"https://raw.githubusercontent.com/NetManAIOps/OmniAnomaly/master/ServerMachineDataset\"\n",
    "\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download the data and labels for each machine to its own folder\n",
    "for machine in machines:\n",
    "    print(f\"Download data of {machine}\")\n",
    "\n",
    "    machine_dir = data_dir / machine\n",
    "    machine_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    data_path = machine_dir / \"data.csv\"\n",
    "    if not data_path.exists():\n",
    "        urllib.request.urlretrieve(f\"{dataset_url}/test/{machine}.txt\", data_path)\n",
    "\n",
    "    labels_path = machine_dir / \"labels.csv\"\n",
    "    if not labels_path.exists():\n",
    "         urllib.request.urlretrieve(f\"{dataset_url}/test_label/{machine}.txt\", labels_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c44fcbc-46a6-495b-860e-c512a9603ec7",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We use Pandas to load the data into a `DataFrame`.\n",
    "Often, data requires cleaning before it can be used, and Pandas si great in this task.\n",
    "However, in this dataset, the data is already cleansed, se we don't need to do any manipulation with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddbbdc9-5576-4fb8-9ed5-c319374d6ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset_df = []\n",
    "\n",
    "for machine in machines:\n",
    "    machine_dir = data_dir / machine\n",
    "\n",
    "    # Read the data and labels\n",
    "    print(f\"Load data of {machine}...\", end=\"\")\n",
    "    df = pd.read_csv(machine_dir / \"data.csv\", header=None).add_prefix(\"f\")\n",
    "    labels = pd.read_csv(machine_dir/ \"labels.csv\", header=None)\n",
    "    df = df.assign(label=labels)\n",
    "    df[\"machine\"] = machine\n",
    "    df[\"timestamp\"] = range(df.shape[0])\n",
    "    print(f\"found {df.shape[0]} events\")\n",
    "\n",
    "    raw_dataset_df.append(df)\n",
    "\n",
    "raw_dataset_df[0].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f50a64-c27e-4cda-afd9-f5da6dcc0276",
   "metadata": {},
   "source": [
    "Next, we convert the dataset into a Temporian `EventSet`. Temporian will help data visualization and feature computation.\n",
    "\n",
    "**Note:** Our anomaly detection model can operate directly on raw time series data, and that's how we'll begin. However, preprocessing and augmenting the temporal data (for example, calculating moving statistics or calendar events) before applying the anomaly detection model can significantly boost its performance. For example of feature engineering, check the [Feature engineering section](https://temporian.readthedocs.io/en/latest/tutorials/m5_competition/#feature-engineering) in the M5 Competition tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c08eb1-4eab-4a15-b27e-084170f42e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = tp.combine(*[tp.from_pandas(x) for x in raw_dataset_df])\n",
    "\n",
    "# Index the EventSet according the the machine name.\n",
    "raw_dataset = raw_dataset.set_index(\"machine\")\n",
    "\n",
    "# Cast the feature and label to a smaller dtypes to same one memory.\n",
    "raw_dataset = raw_dataset.cast(tp.float32).cast({\"label\": tp.int32})\n",
    "\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f191b2a5-3c62-4f2e-aa70-1e9eca600301",
   "metadata": {},
   "source": [
    "Awesome! Seems like each machine has more than 20.000 events and 39 features (counting the \"label\" one).\n",
    "\n",
    "As stated previously, all metrics seem to be anonymized and normalized to `[0, 1]`, so we won't need to take care of that ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7649e96b-028d-4df7-b201-2c48474983c0",
   "metadata": {},
   "source": [
    "## Data visualization\n",
    "\n",
    "Let's take a look at some of the first machine's features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff473e7-e477-4c92-bf27-6692f9df3059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 3 features\n",
    "raw_dataset.plot(indexes=\"machine-1-1\", max_num_plots=3)\n",
    "\n",
    "# Plot the labels\n",
    "raw_dataset[\"label\"].plot(indexes=\"machine-1-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8150c1de-c15f-426b-8e8e-5b9b73901941",
   "metadata": {},
   "source": [
    "Great! A lot to unpack here:\n",
    "- It seems to be easy to understand when an anomaly occurs (label takes value of 1) by looking at the other plots. Features 11 to 14, for example, seem to be very correlated to the label.\n",
    "- The data seems to have some periodicity to it.\n",
    "- Some features seem empty, and we could evaluate dropping them if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0275b3-bc35-4ced-8156-88c187281fdb",
   "metadata": {},
   "source": [
    "## Train / test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ac07ee-2feb-4014-a9ea-cb52327fbe1a",
   "metadata": {},
   "source": [
    "To evaluate the quality of our anomaly detection system, we need to select some test data and remove it from the training.\n",
    "\n",
    "One approach to splitting the dataset would be to use some of the machines for training and the rest for testing (e.g., an 80/20 split). This would be great if all machines were independent. However, if we assume dependencies between machines, this method isn't correct.\n",
    "\n",
    "Instead, we need to split the dataset based on time. We'll use the first 80% of the time series data from all machines for training, and the remaining 20% for testing.\n",
    "\n",
    "Note: Remember that in unsupervised anomaly detection, unlike supervised learning, the training process doesn't rely on labeled data.\n",
    "\n",
    "\n",
    "We'll be creating reusable functions for each step, since we'll do some iteration over the `feature engineering -> training -> evaluation` cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224975e6-0f5e-4ca8-8bf0-c8a2101aacfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_COLS = [\"machine\", \"timestamp\"]\n",
    "\n",
    "def split_train_test(evset: tp.EventSet, train_cutoff: float = 20000):\n",
    "    \"\"\"Splits the dataset in time.\"\"\"\n",
    "\n",
    "    # Compute masks and split data based on cutoff\n",
    "    train_mask = evset.timestamps() <= train_cutoff\n",
    "    test_mask = ~train_mask\n",
    "\n",
    "    # Split EventSets\n",
    "    train_evset = evset.filter(train_mask)\n",
    "    test_evset = evset.filter(test_mask)\n",
    "\n",
    "    print(f\"Train events: {train_evset.num_events()}\")\n",
    "    print(f\"Test events: {test_evset.num_events()}\")\n",
    "\n",
    "    return train_evset, test_evset\n",
    "\n",
    "# Split the data\n",
    "train_dataset, test_dataset = split_train_test(raw_dataset)\n",
    "\n",
    "# Plot the label of the training and testing set for the first 3 machines.\n",
    "tp.plot((\n",
    "    train_dataset[\"label\"].rename(\"training\"),\n",
    "    test_dataset[\"label\"].rename(\"testing\"),\n",
    "), max_num_plots=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31325157-ea11-45a3-a121-4f77ec75b7ba",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Having done all that work to prepare our data, all that remains is to train our model.\n",
    "\n",
    "**Note:** The isolation forest model is configured with `label=\"label\"`.\n",
    "This does not mean the model will be trained with the label (and isolation forest is always trained without labels).\n",
    "Instead, it means we will be able to evaluate the model as a classification model later. If you don't specify `label=\"label\"`, the model will train fine, but you won't be able to evaluate it with `model.evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ba4d6-b875-427f-a649-c41aeb2df21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the input feature names i.e. without the label.\n",
    "feature_names = train_dataset.schema.feature_names()\n",
    "feature_names.remove(\"label\")\n",
    "print(\"feature_names:\", feature_names)\n",
    "\n",
    "# Convert the dataset into a dictionary of numpy arrays.\n",
    "train_dict = tp.to_numpy(train_dataset)\n",
    "\n",
    "# Train the anomaly detection model.\n",
    "model = ydf.IsolationForestLearner(features=feature_names).train(train_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fe0da0-3004-4b95-9946-580af092a110",
   "metadata": {},
   "source": [
    "It is possible to analyze the model. See the [YDF anomaly detection tutorial](https://ydf.readthedocs.io/en/latest/tutorial/anomaly_detection) for more details.\n",
    "In this notebook, we will only focus on the model quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8666cdbe-ca4f-4039-a8b4-1bf9d08226d7",
   "metadata": {},
   "source": [
    "## Evaluation using AUC-ROC\n",
    "\n",
    "We'll be reporting the model's ROC AUC score, which provides an aggregate measure of performance across all possible classification thresholds (since our model outputs an anomaly score for each sample, and in a real-world scenario it would be up to us to define the thershold from which we consider an event to be marked as anomalous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b707ff8-251d-41f9-99cb-6e0d1ab34330",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize=(20,3)\n",
    "\n",
    "results = {}\n",
    "\n",
    "def evaluate(model, test_dataset, name):\n",
    "    \"\"\"Evaluates a model on its training data and unseen test data, computing accuracy score and plotting ground truth vs predictions.\"\"\"\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_dict = tp.to_numpy(test_dataset)\n",
    "    predictions = model.predict(test_dict)\n",
    "    roc_auc = roc_auc_score(test_dict[\"label\"], predictions)\n",
    "\n",
    "    # Store and print the resuls\n",
    "    results[name] = {\"ROC AUC\": roc_auc}\n",
    "    print(\"Results:\")\n",
    "    print(pd.DataFrame(results))\n",
    "\n",
    "    # Convert the predictions (stored as a numpy array) into an eventset alligned with the dataset.\n",
    "    predictions_eventset = tp.event_set(\n",
    "        timestamps=test_dict[\"timestamp\"],\n",
    "        features={\n",
    "            \"predictions\": predictions,\n",
    "            \"machine\": test_dict[\"machine\"],\n",
    "        },\n",
    "        indexes=[\"machine\"],\n",
    "    )\n",
    "\n",
    "    # Plot the anomaly detection score along side the labels.\n",
    "    tp.plot([\n",
    "        predictions_eventset,\n",
    "        test_dataset[\"label\"],\n",
    "    ], max_num_plots=4)\n",
    "\n",
    "evaluate(model, test_dataset, \"raw features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fdb2e3-3fe8-4c5a-abaa-b9693cdd2c3a",
   "metadata": {},
   "source": [
    "That's pretty decent for a first try! Our model seems to be learning.\n",
    "\n",
    "The plotted predictions seem to be all over the place though, with the model sporadically predicting anomaly events throughout in several non-anomalous periods.\n",
    "\n",
    "There's plenty of room for improvement, so let's kick off the feature engineering!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710a04fd-ee6c-46f1-b98e-c6d5a1e9c09d",
   "metadata": {},
   "source": [
    "## Evaluation using Detection AMOC [Advanced]\n",
    "\n",
    "A detection AMOC shows the relation between false alerts (or equivalently, the average time between false alerts) and the average time it takes to detect anomalies.\n",
    "\n",
    "Using Temporian, we compute an AMOC as follows.\n",
    "\n",
    "The objective is to detect anomalous events called \"anomalies\". An anomaly is created each time the \"label\" (or \"target\") is true.\n",
    "Anomalies are detected using a numerical \"signal\" and a \"threshold\". An \"alert\" is created each time the signal is above the threshold.\n",
    "\n",
    "In this dataset, two anomalies that happen close in time can be considered the same anomaly. Therefor, we apply a \"shutoff\" on the anomalies:\n",
    "Two anomalies cannot happen within \"missed_target_shutoff\" of each other. A \"shutoff\" can also be applied on the \"alert\". Instead, we apply the shutoff on the \"false alerts\" (see next definition).\n",
    "\n",
    "A \"false alert\" is an alert **not detecting** an anomaly event, i.e. an alert at time `t` without any anomaly in `[t-anomaly_window, t]`.\n",
    "A \"missed anomaly\" is an anomaly **not detected** by any alert, i.e. an anomaly at time `t` without any alert in `[t, t+anomaly_window]`.\n",
    "The \"time to detection\" of an anomaly at time `t` is the time between the anomaly and the first following alert in `[t, t+abnomal`y_window]`.\n",
    "If not detected by an alert, the \"time to detection\" of an anomaly is related by the \"anomaly_window\" (this makes the plots easier to interpret).\n",
    "\n",
    "Here is a summary:\n",
    "\n",
    "![](https://raw.githubusercontent.com/google/temporian/main/docs/src/assets/amoc.svg)\n",
    "\n",
    "We can compute an AMOC on a model or any other numerical signal. For instance, we can compute the AMOC on a feature.\n",
    "We can find which one of the features `f1`and `f2` alone can best detect anomalies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548cd648-245c-4d34-9f89-a98bb1d307f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define \"compute_detection_amoc\" and \"plot_detection_amoc\" to compute and plot amocs.\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, List\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@dataclass\n",
    "class ThresholdEval:\n",
    "    \"\"\"Model metric for each threshold value applied on the score.\"\"\"\n",
    "\n",
    "    threshold: float\n",
    "    num_false_alerts: int\n",
    "    num_missed_anomalies: int\n",
    "    average_time_to_detection: float\n",
    "    total_record_duration: float\n",
    "    num_machines: int\n",
    "\n",
    "def compute_detection_amoc(target : tp.EventSet,\n",
    "                   signal : tp.EventSet,\n",
    "                   detection_thresholds: List[float] = np.linspace(0,1,100, dtype=np.float32).tolist(),\n",
    "                   anomaly_window : float = 1000.,\n",
    "                   anomaly_shutoff : float = 200.,\n",
    "                   false_alert_shutoff : float = 200.,\n",
    "                   plot_first: bool = True,\n",
    "                  ) -> List[ThresholdEval]:\n",
    "    \"\"\"Computes a detection AMOC.\n",
    "\n",
    "    Args:\n",
    "        target: A boolean feature defining an anomaly to detect.\n",
    "        signal: A numerical feature detecting recent anomalies. A signal above a threshold becomes an alert.\n",
    "        detection_thresholds: List of evaluated detection thresholds applied on the \"signal\".\n",
    "        anomaly_window: For how long an anomaly can be detected.\n",
    "        anomaly_shutoff: Minimum distance between two anomalies.\n",
    "        false_alert_shutoff: Minimum distance between two missed alerts.\n",
    "        plot_first: Plot the alerts, anomalies of the first index and the mid-threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    # List the anomalies\n",
    "    anomalies = target.cast(tp.bool_).filter()\n",
    "    anomalies = anomalies.filter_moving_count(anomaly_shutoff)\n",
    "\n",
    "    # Final timestamp used to collect all the metrics.\n",
    "    end_of_records = signal.end()\n",
    "    end_of_records_global = end_of_records.drop_index(keep=False).end()\n",
    "\n",
    "    def sum_by_index(evset) -> int:\n",
    "        \"\"\"Sums of the last value of a feature in all the indexes.\"\"\"\n",
    "        assert len(evset.schema.features) == 1 # There should be only one feature\n",
    "\n",
    "        return float(evset.drop_index(keep=False).cumsum(sampling=end_of_records_global).get_arbitrary_index_data().features[0][0])\n",
    "\n",
    "    # The duration of record in each index is the time distance between the first and last event.\n",
    "    record_duration = signal.begin().since_last(sampling=signal.end())\n",
    "    total_record_duration = sum_by_index(record_duration)\n",
    "    print(\"total_record_duration:\",total_record_duration)\n",
    "\n",
    "    # There is one index per machine\n",
    "    num_machines = len(target.data)\n",
    "    print(\"num_machines:\",num_machines)\n",
    "\n",
    "    num_anomalies = int(anomalies.drop_index(keep=False).moving_count(np.inf, sampling=end_of_records_global).get_arbitrary_index_data().features[0][0])\n",
    "    print(\"num_anomalies:\",num_anomalies)\n",
    "\n",
    "    # Can also be computed this way:\n",
    "    assert num_anomalies == np.sum([len(machine.timestamps) for key, machine in anomalies.data.items()])\n",
    "\n",
    "    amoc = []\n",
    "\n",
    "    for threshold_idx, threshold in enumerate(detection_thresholds):\n",
    "        is_mid_threshold = threshold_idx == (len(detection_thresholds)//2)\n",
    "\n",
    "        alerts = (signal >= threshold).filter()[[]]\n",
    "\n",
    "        # False alerts\n",
    "        false_alerts = anomalies.moving_count(anomaly_window, sampling=alerts).equal(0).filter()\n",
    "        false_alerts = false_alerts.filter_moving_count(false_alert_shutoff)\n",
    "        num_false_alerts = false_alerts.moving_count(np.inf, sampling=end_of_records)\n",
    "\n",
    "        # Missed targets\n",
    "        missed_anomalies = alerts.moving_count(anomaly_window, sampling=anomalies.lag(anomaly_window)).equal(0).filter().leak(anomaly_window)\n",
    "        missed_anomalies = missed_anomalies.filter_moving_count(false_alert_shutoff)\n",
    "        num_missed_anomalies = missed_anomalies.moving_count(np.inf, sampling=end_of_records)\n",
    "\n",
    "        # Time to detection\n",
    "        time_to_detection = anomalies.until_next(sampling=alerts, timeout=anomaly_window)\n",
    "        # Set the time to detection of non detected anomalies with the maximum anomaly_window.\n",
    "        time_to_detection = time_to_detection.isnan().where(anomaly_window, time_to_detection)\n",
    "        # Note: Some machines don't have alerts\n",
    "        sum_time_to_detection = time_to_detection.cumsum(sampling=end_of_records)\n",
    "\n",
    "        if plot_first and is_mid_threshold:\n",
    "            anomalies.name = \"anomalies\"\n",
    "            alerts.name = \"alerts\"\n",
    "            missed_anomalies.name = \"missed_anomalies\"\n",
    "            false_alerts.name = \"false_alerts\"\n",
    "            tp.plot([\n",
    "                target,\n",
    "                signal,\n",
    "                anomalies,\n",
    "                alerts,\n",
    "                missed_anomalies,\n",
    "                false_alerts\n",
    "                ],\n",
    "                    indexes=\"machine-1-1\",\n",
    "                   height_per_plot_px=100)\n",
    "\n",
    "        amoc.append(ThresholdEval(\n",
    "            threshold=threshold,\n",
    "            num_false_alerts=sum_by_index(num_false_alerts) / num_machines,\n",
    "            num_missed_anomalies=sum_by_index(num_missed_anomalies) / num_machines,\n",
    "            average_time_to_detection=sum_by_index(sum_time_to_detection) / num_anomalies,\n",
    "            total_record_duration=total_record_duration / num_machines,\n",
    "            num_machines=num_machines,\n",
    "        ))\n",
    "\n",
    "    return amoc\n",
    "\n",
    "def plot_detection_amoc(amoc_dict: Dict[str, List[ThresholdEval]]):\n",
    "\n",
    "    # Plot the AMOC and other related plots\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(5*2, 3), squeeze=False)\n",
    "\n",
    "    for name, amoc in amoc_dict.items():\n",
    "        ax = axs[0,0]\n",
    "        ax.plot([e.average_time_to_detection for e in amoc],\n",
    "                [e.num_false_alerts for e in amoc],\n",
    "               label=name)\n",
    "\n",
    "        ax = axs[0,1]\n",
    "        ax.plot([e.average_time_to_detection for e in amoc if e.num_false_alerts != 0],\n",
    "                [e.total_record_duration / e.num_false_alerts for e in amoc if e.num_false_alerts != 0],\n",
    "               label=name)\n",
    "\n",
    "    ax = axs[0,0]\n",
    "    ax.set_xlabel(\"Avg. time to detection\")\n",
    "    ax.set_ylabel(\"Num. false alerts\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax = axs[0,1]\n",
    "    ax.set_xlabel(\"Avg. time to detection\")\n",
    "    ax.set_ylabel(\"Avg. time between false alerts\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.invert_xaxis()\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "amoc_f1 = compute_detection_amoc(raw_dataset[\"label\"], raw_dataset[\"f1\"])\n",
    "amoc_f2 = compute_detection_amoc(raw_dataset[\"label\"], raw_dataset[\"f2\"], plot_first=False)\n",
    "\n",
    "plot_detection_amoc({\n",
    "    \"f1\":amoc_f1,\n",
    "    \"f2\":amoc_f2,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d945ca5-0962-4aef-8054-12c02a953709",
   "metadata": {},
   "source": [
    "We can see that both features performs relatively equally.\n",
    "For instance, with an average time to detection of 400 time-units, both features generate approximately ~30 false alerts for each machine.\n",
    "\n",
    "What about our detection model now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9b07f3-99e3-445c-8fd2-9c9a83ebf1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = tp.to_numpy(test_dataset)\n",
    "pred_evset = tp.event_set(\n",
    "        timestamps=test_dict[\"timestamp\"],\n",
    "        features={\n",
    "            \"predictions\": model.predict(test_dict),\n",
    "            \"label\": test_dict[\"label\"],\n",
    "            \"machine\": test_dict[\"machine\"],\n",
    "        },\n",
    "        indexes=[\"machine\"],\n",
    "    )\n",
    "\n",
    "amoc_model_raw_features = compute_detection_amoc(pred_evset[\"label\"], pred_evset[\"predictions\"])\n",
    "\n",
    "plot_detection_amoc({\n",
    "    \"f1\":amoc_f1,\n",
    "    \"f2\":amoc_f2,\n",
    "    \"model\": amoc_model_raw_features,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3b0d3a-34be-4bbd-aa9e-65b10f7ee11c",
   "metadata": {},
   "source": [
    "The model is **much** better than the features alone. This is expected, but it is a good check to do :).\n",
    "\n",
    "For example, the model is able to detect the anomalies within 200 time-units while raising ~8 false alerts.\n",
    "By comparison, using `f1`or `f2`alone will generate ~55 false alerts for the same time to detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17562ccf-ce6c-4185-9d3c-2b4bf274aed0",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "In this notebook we learned how to perform feature engineering and visualization using Temporian, applying it to a real-world anomaly detection use case.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
