{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a8e59c2-df0a-4782-8ff7-514b826f76bc",
   "metadata": {},
   "source": [
    "# Detecting payment card fraud with Temporian and TensorFlow Decision Forests\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/temporian/blob/main/docs/src/tutorials/bank_fraud_detection_with_tfdf.ipynb)\n",
    "\n",
    "Detection of fraud in online banking is critical for banks, businesses, and their consumers. The book \"[Reproducible Machine Learning for Credit Card Fraud Detection](https://fraud-detection-handbook.github.io/fraud-detection-handbook/Foreword.html)\" by Le Borgne et al. introduces the problem of payment card fraud and shows how fraud can be detected using machine learning. However, since banking transactions are sensitive and not widely available, the book uses a synthetic dataset for practical exercises.\n",
    "\n",
    "This notebook uses the same dataset to show how to use Temporian and [TensorFlow Decision Forests](https://www.tensorflow.org/decision_forests) to detect fraud. Temporian is used for data preprocessing and augmentation, while TensorFlow Decision Forests is used for model training. Data augmentation is often critical for temporal data, and this notebook demonstrates how complex data augmentation can be performed with ease using Temporian.\n",
    "\n",
    "The notebook is divided into three parts:\n",
    "\n",
    "1. Download the dataset and load it as a Temporian EventSet.\n",
    "1.  Perform various types of augmentations and visualize the correlation between the augmented features and fraud target labels.\n",
    "1. Train and evaluate a machine learning model to detect fraud using the augmented features.\n",
    "\n",
    "\n",
    "*Note: This notebook assumes a basic understanding of Temporian. If you are not familiar with Temporian, we recommend that you read the [3 minutes guide to Temporian](https://temporian.readthedocs.io/en/latest/3_minutes) guide first.*\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ce89356-af4a-4ddb-9f37-5e57ffe7ef5a",
   "metadata": {},
   "source": [
    "## Install and import dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc817d-b6fa-4b89-af98-efadcfb30324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data preprocessing and augmantation\n",
    "%pip install temporian -q\n",
    "\n",
    "# For model training\n",
    "%pip install tensorflow tensorflow_decision_forests -q\n",
    "\n",
    "# To plots the results\n",
    "%pip install seaborn -q\n",
    "\n",
    "# To compute the ROC curve of the model\n",
    "%pip install scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a2569-03ae-4ead-a16c-e9e6ef5cf5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import concurrent.futures\n",
    "\n",
    "import pandas as pd\n",
    "import temporian as tp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow_decision_forests as tfdf\n",
    "from sklearn import metrics as sk_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ab4a8-db54-47f3-a212-5ef785fa607b",
   "metadata": {},
   "source": [
    "The dataset consists of banking transactions sampled between April 1, 2018 and September 30, 2018. The transactions are stored in CSV files, one for each day. The transactions from April 1, 2018 to August 31, 2018 (inclusive) are used for training, while the transactions from September 1, 2018 to September 30, 2018 are used for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e8425f-bdc1-4a0e-8bbf-96af7568811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.date(2018, 4, 1)\n",
    "end_date = datetime.date(2018, 9, 30)\n",
    "train_test_split = datetime.datetime(2018, 9, 1)\n",
    "\n",
    "# Note: You can reduce the end and train/test split dates to speed-up the notebook execution.\n",
    "\n",
    "# List the input csv files\n",
    "filenames = []\n",
    "while start_date <= end_date:\n",
    "    filenames.append(f\"{start_date}\")\n",
    "    start_date += datetime.timedelta(days=1)\n",
    "print(f\"{len(filenames)} dates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d796546-49c9-49c2-b8ef-db6818b8e5c5",
   "metadata": {},
   "source": [
    "The dataset is downloaded and converted into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9496610-0e6a-4f73-a0a3-769146801f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_date(filename):\n",
    "    print(\".\",end=\"\", flush=True)\n",
    "    return pd.read_pickle(f\"https://github.com/Fraud-Detection-Handbook/simulated-data-raw/raw/main/data/{filename}.pkl\")\n",
    "\n",
    "print(\"Downloading dataset\",end=\"\")\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    frames = executor.map(load_date, filenames)\n",
    "dataset_pd = pd.concat(frames)\n",
    "print(\"done\")\n",
    "print(f\"Found {len(dataset_pd)} transactions\")\n",
    "\n",
    "dataset_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6f3b55-1fa8-424f-a0e7-9a8cfc53bb08",
   "metadata": {},
   "source": [
    "We only keep the following columns of interest:\n",
    "\n",
    "- **TX_DATETIME**: The date and time of the transaction.\n",
    "- **CUSTOMER_ID**: The unique identifier of the customer.\n",
    "- **TERMINAL_ID**: The identifier of the terminal where the transaction was made.\n",
    "- **TX_AMOUNT**: The amount of the transaction.\n",
    "- **TX_FRAUD**: Whether the transaction is fraudulent (1) or not (0).\n",
    "\n",
    "Our goal is to predict whether a transaction is fraudulent at the time of the transaction, using only the information from this transaction and previous transactions. The information about whether a transaction is fraudulent is not known at the time of the transaction. Instead, it is only known one week after the transaction. While this is too late to prevent the fraudulent transaction, it is available for future transactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245659eb-0a36-4dfd-ae22-260ed478cf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pd = dataset_pd[[\"TX_DATETIME\", \"CUSTOMER_ID\", \"TERMINAL_ID\", \"TX_AMOUNT\", \"TX_FRAUD\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47568218-a7c0-4f2b-8de6-4a738ec1c23f",
   "metadata": {},
   "source": [
    "Convert the Pandas DataFrame into a Temporian EventSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df5afca-39bd-4a95-b1db-f0ceffb0fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tp = tp.from_pandas(dataset_pd, timestamps=\"TX_DATETIME\")\n",
    "\n",
    "dataset_tp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad876f3f-2bae-4db4-ac1e-0d371ef4d437",
   "metadata": {},
   "source": [
    "We can plot the whole dataset, but the resulting plot will be very busy because all the transactions are currently grouped together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c5aed2-ec03-415e-8ce7-efae10f4b73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2477a90d-6ddd-4da8-91e8-9b7cae085537",
   "metadata": {},
   "source": [
    "Instead, we can plot the transaction of a single client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d33be4-458c-493c-88d8-9072a82ef792",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.add_index(dataset_tp, \"CUSTOMER_ID\").plot(indexes=\"3774\")\n",
    "\n",
    "# Same plot as:\n",
    "# tp.filter(dataset_tp, tp.equal(dataset_tp[\"CUSTOMER_ID\"], \"3774\")).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f4a4fb-fc01-48a8-b69c-6fdf96ea65eb",
   "metadata": {},
   "source": [
    "After exploring the dataset, we want to compute some augmented features that may correlate with fraudulent activities. We will compute the following three features:\n",
    "\n",
    "**Calendar features**: We will extract the hour of the day and the day of the week as individual features. This is because fraudulent transactions may be more likely to occur at specific times.\n",
    "\n",
    "**Moving sum of fraud per customer**: For each client, we will extract the number of fraudulent transactions in the last 4 weeks. This is because clients who start to commit fraud (maybe the a card was stolen) may be more likely to commit fraud in the future. However, since we only know after a week if a transaction is fraudulent, there will be a lag in this feature.\n",
    "\n",
    "**Moving sum of fraud per terminal**: For each terminal, we will extract the number of fraudulent transactions in the last 4 weeks. This is because some fraudulent transactions may be caused by ATM skimmers. In this case, many transactions from the same terminal may be fraudulent. However, since we only know after a week if a transaction is fraudulent, there will be a lag in this feature as well.\n",
    "\n",
    "Data augmentation features often have parameters that need to be selected. For example, why look at the last 4 weeks instead of the last 8 weeks? In practice, you will want to compute the features with many different parameter values (e.g., 1 day, 2 days, 1 week, 2 weeks, 4 weeks, and 8 weeks). However, to keep this example simple, we will only use 4 weeks here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5a8647-21ec-42c0-9af0-149727037ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tp.compile\n",
    "def augment_transactions(transactions: tp.EventSetNode) -> tp.EventSetNode:\n",
    "    print(\"TRANSACTIONS:\\n\", transactions.schema, sep = '')\n",
    "\n",
    "    # Create a unique ID for each transaction.\n",
    "    transaction_id = tp.rename(tp.enumerate(transactions), \"transaction_id\")\n",
    "    transactions = tp.glue(transactions, transaction_id)\n",
    "\n",
    "    # 1.\n",
    "    # Hour of day and day of week of each transaction.\n",
    "    calendar = tp.glue(\n",
    "        tp.calendar_hour(transactions),\n",
    "        tp.calendar_day_of_week(transactions),\n",
    "        transactions[\"transaction_id\"],\n",
    "    )\n",
    "    print(\"CALENDAR:\\n\", calendar.schema, sep = '')\n",
    "\n",
    "    # 2.\n",
    "    # Index the transactions per customer\n",
    "    per_customer = tp.add_index(transactions, \"CUSTOMER_ID\")\n",
    "    # Lag the fraud by 1 week\n",
    "    lagged_fraud_per_customer = tp.lag(per_customer[\"TX_FRAUD\"], tp.duration.weeks(1))\n",
    "    # Moving sum of transactions over the last 4 weeks\n",
    "    feature_per_customer = tp.moving_sum(lagged_fraud_per_customer, tp.duration.weeks(4), sampling=per_customer)\n",
    "    # Rename the feature for book-keeping\n",
    "    feature_per_customer = tp.rename(feature_per_customer, \"per_customer.moving_sum_frauds\")\n",
    "    # Aggregate the newly computed feature with the ther customer features.\n",
    "    feature_per_customer = tp.glue(feature_per_customer, per_customer)\n",
    "    # Print the schema\n",
    "    print(\"PER CUSTOMER:\\n\", per_customer.schema, sep = '')\n",
    "\n",
    "    # 3.\n",
    "    # The moving sum of fraud per terminal is similar to the moving sum per customer.\n",
    "    # Instead of indexing by customer, the dataset is indexed by terminal.\n",
    "    per_terminal = tp.add_index(transactions, \"TERMINAL_ID\")\n",
    "    lagged_fraud_per_terminal = tp.lag(per_terminal[\"TX_FRAUD\"], tp.duration.weeks(1))\n",
    "    feature_per_terminal = tp.moving_sum(lagged_fraud_per_terminal, tp.duration.weeks(4), sampling=per_terminal)\n",
    "    feature_per_terminal = tp.rename(feature_per_terminal, \"per_terminal.moving_sum_frauds\")\n",
    "    feature_per_terminal = tp.glue(feature_per_terminal, per_terminal)\n",
    "    print(\"PER TERMINAL:\\n\", per_terminal.schema, sep = '')\n",
    "\n",
    "    # Join the per customer and per terminal features\n",
    "    augmented_transactions = tp.join(\n",
    "        tp.drop_index(feature_per_terminal),\n",
    "        tp.drop_index(feature_per_customer)[[\"per_customer.moving_sum_frauds\",\"transaction_id\"]],\n",
    "        on=\"transaction_id\")\n",
    "    \n",
    "    # Join the calendar features\n",
    "    augmented_transactions = tp.join(\n",
    "        augmented_transactions,\n",
    "        calendar[[\"calendar_hour\", \"calendar_day_of_week\", \"transaction_id\"]],\n",
    "        on=\"transaction_id\")\n",
    "    \n",
    "    print(\"AUGMENTED TRANSACTIONS:\\n\", augmented_transactions.schema)\n",
    "\n",
    "    return augmented_transactions\n",
    "\n",
    "# Compute the augmanted features\n",
    "augmented_dataset_tp = augment_transactions(dataset_tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670dcdb5-93d4-495d-a852-5d37dc0a364e",
   "metadata": {},
   "source": [
    "Plot the augmented features on the selected customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d2cef6-56bf-49d4-8b10-fe44b47df616",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.add_index(augmented_dataset_tp, \"CUSTOMER_ID\").plot(indexes=\"3774\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bd7b19-f7f4-44f6-b6c9-2baa0ce6c85d",
   "metadata": {},
   "source": [
    "Save the Temporian program to compute the augmented transactions. We will not use this program again in this notebook, but in practice, this data augmentation stage should be included with the model.\n",
    "\n",
    "A saved Temporian program can also be applied on a large dataset that does not fit in memory using the Beam backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3906c313-2c8e-4ab6-89c8-e5fe80c66ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.save(augment_transactions, inputs={\"transactions\":dataset_tp.schema}, path=\"/tmp/augment_transactions.tempo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74afcc35-2103-4b13-8513-fa76dcfbf15b",
   "metadata": {},
   "source": [
    "Convert the Temporian EventSet into a Pandas DataFrame and plot the relation between the augmented features and the label.\n",
    "\n",
    "**Observations:** The feature `per_terminal.moving_sum_frauds` and `per_customer.moving_sum_frauds` seems to discriminate between fraudulent and non-fraudulent transactions, while the calendar features are not discriminative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099c17ac-6efd-4409-9b94-4914a7095ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(10, 8))\n",
    "\n",
    "sns.ecdfplot(data=augmented_dataset_pd, x=\"per_terminal.moving_sum_frauds\", hue=\"TX_FRAUD\", ax=axs[0,0])\n",
    "sns.ecdfplot(data=augmented_dataset_pd, x=\"per_customer.moving_sum_frauds\", hue=\"TX_FRAUD\", ax=axs[1,0])\n",
    "sns.ecdfplot(data=augmented_dataset_pd, x=\"calendar_hour\", hue=\"TX_FRAUD\", ax=axs[0,1])\n",
    "sns.ecdfplot(data=augmented_dataset_pd, x=\"calendar_day_of_week\", hue=\"TX_FRAUD\", ax=axs[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c52af7-be82-45ee-a6a3-df7fbf90f635",
   "metadata": {},
   "source": [
    "The next step is to split the dataset into a training and testing dataset.\n",
    "\n",
    "One common approach is to use the `tp.timestamps` operator. This operator converts the timestamp of a transaction into a feature that can be compared to `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c005c2-c59c-4c25-abc7-f0fe4727145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = tp.timestamps(augmented_dataset_tp) < train_test_split.timestamp()\n",
    "is_test = tp.invert(is_train)\n",
    "\n",
    "# Plot\n",
    "is_train = tp.rename(is_train, \"is_train\")\n",
    "is_test = tp.rename(is_test, \"is_test\")\n",
    "tp.plot([is_train, is_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd397bfb-8be1-4a4d-8613-9c594cf1eac4",
   "metadata": {},
   "source": [
    "An alternative and equivalent solution is to create a demarcating event that separates the training and testing examples. We can then use the `tp.since_last` and `tp.isnan` functions to compute for each transaction whether the demarcating event has already been seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3228ef6e-f2e2-4d22-a488-f5f76d0bb39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a demarcating event.\n",
    "train_test_switch_tp = tp.event_set(timestamps=[train_test_split])\n",
    "\n",
    "# Plot\n",
    "train_test_switch_tp.plot()\n",
    "\n",
    "# All the transactions before the demarcating event are part of the training dataset (i.e. `is_train=True`) \n",
    "is_train = tp.isnan(tp.since_last(train_test_switch_tp, augmented_dataset_tp))\n",
    "is_test = tp.invert(is_train)\n",
    "\n",
    "# Plot\n",
    "is_train = tp.rename(is_train, \"is_train\")\n",
    "is_test = tp.rename(is_test, \"is_test\")\n",
    "tp.plot([is_train, is_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8726c272-144e-427c-a965-466f7683b1ed",
   "metadata": {},
   "source": [
    "We can now split the dataset into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d10b205-aef8-4688-97b2-5e9f0ae35582",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset_train_tp = tp.filter(augmented_dataset_tp, is_train)\n",
    "augmented_dataset_test_tp = tp.filter(augmented_dataset_tp, is_test)\n",
    "\n",
    "# Print the schema of the training dataset\n",
    "augmented_dataset_train_tp.schema.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1182fb3d-f39f-4b76-8e62-b19a29c91998",
   "metadata": {},
   "source": [
    "We first convert the Temporal EventSets into Pandas DataFrames. Then, we use the `tfdf.keras.pd_dataframe_to_tf_dataset` function to convert these DataFrames into TensorFlow datasets that can be used by TensorFlow Decision Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e72c9-f36c-409a-baca-cb7c3d43d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporian EventSet to Pandas DataFrame\n",
    "dataset_train_pd = tp.to_pandas(augmented_dataset_train_tp)\n",
    "dataset_test_pd = tp.to_pandas(augmented_dataset_test_tp)\n",
    "\n",
    "print(f\"Train example: {len(dataset_train_pd)}\")\n",
    "print(f\"Test example: {len(dataset_test_pd)}\")\n",
    "\n",
    "# Pandas DataFrame to TensorFlow Dataset\n",
    "dataset_train_tf = tfdf.keras.pd_dataframe_to_tf_dataset(dataset_train_pd, label=\"TX_FRAUD\")\n",
    "dataset_test_tf = tfdf.keras.pd_dataframe_to_tf_dataset(dataset_test_pd, label=\"TX_FRAUD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb54b03a-5e72-4873-9bcb-4125930f04f5",
   "metadata": {},
   "source": [
    "We can then train a TF-DF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5425639-7bef-4f5d-b7ec-c6555987471d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tfdf.keras.GradientBoostedTreesModel(features=[tfdf.keras.FeatureUsage(\"per_customer.moving_sum_frauds\"),\n",
    "                                                       tfdf.keras.FeatureUsage(\"per_terminal.moving_sum_frauds\"),\n",
    "                                                       tfdf.keras.FeatureUsage(\"calendar_hour\"),\n",
    "                                                       tfdf.keras.FeatureUsage(\"calendar_day_of_week\"),\n",
    "                                                      ],\n",
    "                                             exclude_non_specified_features=True)\n",
    "model.fit(dataset_train_tf, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f473ac-0746-4610-ad93-cae58ed0d336",
   "metadata": {},
   "source": [
    "Finally, we plot the ROC (Receiver operating characteristic) curve and compute the AUC (Area Under the Curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5e0db4-23be-46d6-b18c-5ac43e2d1d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions of the model\n",
    "test_predictions = model.predict(dataset_test_tf, verbose=0)[:,0]\n",
    "\n",
    "# The real fraud information\n",
    "test_labels = dataset_test_pd[\"TX_FRAUD\"].values\n",
    "\n",
    "# Compute the ROC and AUC.\n",
    "fpr, tpr, thresholds = sk_metrics.roc_curve(test_labels, test_predictions)\n",
    "auc = sk_metrics.roc_auc_score(test_labels, test_predictions)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (AUC = %0.3f)'  % auc )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7e9b0a-8a13-42af-b5e5-6a9614c6787b",
   "metadata": {},
   "source": [
    "The augmented features we created are efficient at identifying some types of fraud, as evidenced by the point (FPR=0.02, TPR=0.5).\n",
    "\n",
    "However, for FPRs greater than 0.02, the TPR increases slowly, indicating that the remaining types of fraud are more difficult to detect. We need to conduct further analysis and create new features to improve our ability to detect these remaining frauds.\n",
    "\n",
    "Do you have any ideas for other features or feature augmentations that could improve the model's performance? For example, we could compute features per customer and per terminal, or we could create features related to transaction amount. These changes could help us reach an AUC of >0.88.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
